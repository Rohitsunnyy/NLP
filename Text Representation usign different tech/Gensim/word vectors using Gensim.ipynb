{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api \n",
    "\n",
    "wv = api.load(\"word2vec-google-news-300\") #https://github.com/piskvorky/gensim-data  (Can refer this for other tech were it is trained on vast like (Twitter, wiki, google articles etc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying out some functions in this gensim :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7647869"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.similarity(w1=\"great\", w2=\"wonderful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Puppy', 0.704573929309845),\n",
       " ('dog', 0.6978708505630493),\n",
       " ('Dogs', 0.694400429725647),\n",
       " ('dogs', 0.6479228734970093),\n",
       " ('Pet', 0.6227257251739502),\n",
       " ('Canine', 0.6188513040542603),\n",
       " ('Poodle', 0.6097930073738098),\n",
       " ('Pooches', 0.6097025275230408),\n",
       " ('Cat', 0.6061108112335205),\n",
       " ('Golden_Retriever', 0.5951071977615356)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(\"Dog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do something like this using postive and negative words :\n",
    "\n",
    "King - man + woman = Queen \n",
    "\n",
    "France - Paris + Berlin = Germany "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Germany', 0.7901254892349243),\n",
       " ('Austria', 0.6026812195777893),\n",
       " ('German', 0.6004959940910339),\n",
       " ('Germans', 0.5851002335548401),\n",
       " ('Poland', 0.5847075581550598),\n",
       " ('Hungary', 0.5271855592727661),\n",
       " ('BBC_Tristana_Moore', 0.5249711275100708),\n",
       " ('symbol_RSTI', 0.5245768427848816),\n",
       " ('Belgium', 0.5221248269081116),\n",
       " ('Germnay', 0.5199406147003174)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(positive=[\"France\", \"Berlin\"], negative=[\"Paris\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cat'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.doesnt_match([\"Facebook\", \"cat\", \"google\", \"Meta\"]) #this will tell which of the word is not matching here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 104.8/104.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "#loading an other model:\n",
    "\n",
    "glv = api.load(\"glove-twitter-25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('too', 0.9648017287254333),\n",
       " ('day', 0.9533665180206299),\n",
       " ('well', 0.9503171443939209),\n",
       " ('nice', 0.9438972473144531),\n",
       " ('better', 0.9425961971282959),\n",
       " ('fun', 0.9418926239013672),\n",
       " ('much', 0.9413353800773621),\n",
       " ('this', 0.9387555122375488),\n",
       " ('hope', 0.9383508563041687),\n",
       " ('great', 0.9378516674041748)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glv.most_similar(\"good\")  #As you can see it desnt match like previous model as this has been trained with twitter data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'central'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glv.doesnt_match(\"Breakfast dinner central lunch brunch\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As you can see their are different trained model like Spacy, Gensim, Pytorch , we can load this vectores using this libaraies and use them accordign to our needs "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
